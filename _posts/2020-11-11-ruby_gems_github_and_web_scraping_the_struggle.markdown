---
layout: post
title:      "Ruby Gems, Github, and Web Scraping: The Struggle"
date:       2020-11-11 08:31:17 +0000
permalink:  ruby_gems_github_and_web_scraping_the_struggle
---


For my most recent project at Flatiron School, I got to make a project based on either web scraping, or using a websiteâ€™s built in API to gather data, then run a CLI to display the data based on the users choices. Out of my two options, I decided to do web scraping as it not only opened up more options for me, I was also more experienced with using gems like Nokogiri than I was handling JSON and APIs. At this point, I was ready to get my project started! At first, I looked into what I wanted to create with web scrapers and decided on a news scraper that would get articles about technology, health and politics. Unsure of which sites to pick, I just went with the first three that came off the top of my head: Business Insider, CNN and Fox News. The next day, I woke up, grabbed the tech news from the websites, programmed for about 45 minutes and then proceeded to immediately lose internet access. This was due to the recent hurricane Zeta that hit my area, and knocked out the internet for everyone in my area. Fortunately for me, I had spent the past few days researching CLIs, and how to appropriately use classes to store class variables with the objects of information I need to display. For the next five days, I would finish the majority of my scraper class for my project. After the internet finally came back up, I was able to start the debugging process, where I quickly found out that I had a lot of fixing to do ahead of me. The code wouldn't work at all, the scrapers weren't directed at the proper sites yet as I couldn't load them without the internet, and my repository was fairly out of date because I wasn't able to commit changes when coding in notepad. About halfway through my debugging, I messed up my commit and accidentally deleted all of my code in the repository! Unsure of how this happened, I spent an entire day trying to figure out what went wrong and how I could undo my mistakes. In the end, I made a new branch that had all of the previous commits, made it the default branch and deleted my "main" branch. One day later, I finally finished my code for the scrapers. I eagerly filmed myself programming the CLI, tested it to make sure everything worked, and then was ready to turn in my code! ...or so I thought, as I still had not created the gem at the time, and had absolutely no clue how to do so. Another two days go by, and I slowly learned how to create gems, and push them onto rubygems.org. After a few mistakes and two versions later, it is finally up and available at https://rubygems.org/gems/news_helper! Despite all the stress and anxiety that these series of events have brought me, I'm very glad I was able to get through all of this and come out with much more knowledge than I had beforehand. The skills I have learned for this project have a long way to go, but these fundamentals will be useful for future development, and I can't wait to put them to good use!
